{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fd3cf3a",
   "metadata": {},
   "source": [
    "# PCA (Principal Component Analysis) – Step-by-Step\n",
    "PCA is an **unsupervised** technique that reduces the number of features while keeping as much **variance (information)** as possible.\n",
    "\n",
    "In this notebook you will:\n",
    "1) Create multi-feature data\n",
    "2) Scale the data\n",
    "3) Reduce dimensions using PCA\n",
    "4) Inspect explained variance\n",
    "5) Visualize the 2D PCA projection\n",
    "6) (Bonus) Use PCA before K-Means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b6cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440672c3",
   "metadata": {},
   "source": [
    "## 1) Create multi-feature data\n",
    "We'll create 3D data with two natural groups (for visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d41279",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Two clusters in 3D\n",
    "A = rng.normal(loc=[0, 0, 0], scale=[1.0, 0.7, 0.5], size=(60, 3))\n",
    "B = rng.normal(loc=[4, 3, 2], scale=[1.0, 0.7, 0.5], size=(60, 3))\n",
    "X = np.vstack([A, B])\n",
    "\n",
    "print('Original shape:', X.shape)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0187fb2c",
   "metadata": {},
   "source": [
    "## 2) Scale the data (important for PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff43d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print('First row before:', X[0])\n",
    "print('First row after :', X_scaled[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e49d6",
   "metadata": {},
   "source": [
    "## 3) Apply PCA (reduce from 3D → 2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ae9711",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print('New shape:', X_pca.shape)\n",
    "X_pca[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cd9982",
   "metadata": {},
   "source": [
    "## 4) Explained variance (how much information is kept?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c104af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Explained variance ratio:', pca.explained_variance_ratio_)\n",
    "print('Total variance kept:', pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b61d403",
   "metadata": {},
   "source": [
    "## 5) Visualize the 2D PCA projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21719c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA Projection (3D → 2D)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf5d66f",
   "metadata": {},
   "source": [
    "## 6) How many components should we keep?\n",
    "Plot cumulative explained variance to decide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19968ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_full = PCA().fit(X_scaled)\n",
    "cum = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(cum) + 1), cum, marker='o')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.title('How many PCA components to keep?')\n",
    "plt.ylim(0, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f1032",
   "metadata": {},
   "source": [
    "## 7) Bonus: PCA before K-Means\n",
    "Often you reduce dimensions first, then cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850e7a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels)\n",
    "plt.scatter(centers[:, 0], centers[:, 1], marker='X', s=250)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('K-Means Clustering on PCA Output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d56e65",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- PCA is **linear** (best when relationships are roughly linear).\n",
    "- PCA helps with visualization and speed, but may lose some information.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}